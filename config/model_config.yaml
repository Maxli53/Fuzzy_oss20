# GPT-OSS 20B Model Configuration
model:
  name: "gpt-oss-20b"
  quantization: "8bit"
  context_length: 2048
  framework: "transformers"

# LoRA Configuration
lora:
  r: 16                    # Low-rank dimension
  alpha: 32               # Scaling parameter
  dropout: 0.1            # Regularization
  target_modules:
    - "q_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
  task_type: "CAUSAL_LM"  # Autoregressive language modeling

# Training Configuration
training:
  batch_size: 4
  gradient_accumulation: 8
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_steps: 500
  max_epochs: 10
  evaluation_strategy: "steps"
  eval_steps: 500

# Generation Parameters
generation:
  max_length: 512
  temperature: 0.7
  do_sample: true
  pad_token_id: null  # Will be set from tokenizer